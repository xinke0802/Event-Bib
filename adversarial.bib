@InProceedings{zhao-dua-singh:2018:ICLR2018,
  author    = {Zhao, Zhengli and Dua, Dheeru and Singh, Sameer},
  title     = {Generating Natural Adversarial Examples},
  booktitle = {Proceedings of the Sixth International Conference on Learning Representations},
  month     = {May},
  year      = {2018},
  address   = {Vancouver, Canada},
  url       = {https://openreview.net/pdf?id=H1BLjgZCb}
}

@InProceedings{Goodfellow:2015:ICLR2015,
  author = {Goodfellow, Ian and Shlens, Jonathon and Szegedy, Christian},
  title = {{Explaining and Harnessing Adversarial Examples}},
  booktitle = {Proceedings of the Third International Conference on Learning Representations},
  month     = {May},
  year      = {2015},
  address   = {Vancouver, Canada},
  url       = {https://arxiv.org/pdf/1412.6572.pdf}
}

@InProceedings{Jiang-etal:2009:IJCNN2009,
  author    = {Jiang, Yulei and Zur, Richard M. and Pesce, Lorenzo L. and Drukker Karen},
  title     = {A study of the effect of noise injection on the training of artificial neural networks},
  booktitle = {Proceedings of the International Joint Conference on Neural Networks on 2009},
  month     = {June},
  year      = {2009},
  address   = {Atlanta, GA, USA},
  publisher = {IEEE},
  pages     = {1428--1432},
  url       = {https://ieeexplore.ieee.org/document/5178981}
}

@InProceedings{li-cohn-baldwin:2017:EACLshort,
  author    = {Li, Yitong  and  Cohn, Trevor  and  Baldwin, Timothy},
  title     = {Robust Training under Linguistic Adversity},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},
  month     = {April},
  year      = {2017},
  address   = {Valencia, Spain},
  publisher = {Association for Computational Linguistics},
  pages     = {21--27},
  abstract  = {Deep neural networks have achieved remarkable results across many language
	processing tasks, however they have been shown to be susceptible to overfitting
	and highly sensitive to noise, including adversarial attacks. In this work, we
	propose a linguistically-motivated  approach for training robust models based
	on exposing the model to corrupted text examples at training time. We consider
	several flavours of linguistically plausible corruption, include lexical
	semantic and syntactic methods. Empirically, we evaluate our method with a
	convolutional neural model across a range of sentiment analysis datasets.
	Compared with a baseline and the dropout method, our method achieves better
	overall performance.},
  url       = {http://www.aclweb.org/anthology/E17-2004}
}

