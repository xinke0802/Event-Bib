@inproceedings{LiuEtal:2018:AAAI2018,
 author = {Liu, Jian and Chen, Yubo and Liu, Kang and Zhao, Jun},
 title = {Event Detection via Gated Multilingual Attention Mechanism},
 booktitle = {The Thirty-Second AAAI Conference on Artificial Intelligence},
 series = {AAAI '2018},
 year = {2018},
 location = {New Orleans, Louisiana, USA},
 pages = {4865-4872},
 url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16371},
 publisher = {Association for the Advancement of Artificial Intelligence},
 address = {New York, NY, USA},
 abstract = {Identifying event instance in text plays a critical role in building NLP applications such as Information Extraction (IE) system. 
              However, most existing methods for this task focus only on monolingual clues of a specific language and ignore the massive 
              information provided by other languages. Data scarcity and monolingual ambiguity hinder the performance of these monolingual 
              approaches. In this paper, we propose a novel multilingual approach---dubbed as Gated Multilingual Attention (GMLATT) 
              framework---to address the two issues simultaneously. In specific, to alleviate data scarcity problem, we exploit the 
              consistent information in multilingual data via context attention mechanism. Which takes advantage of the consistent 
              evidence in multilingual data other than learning only from monolingual data. To deal with monolingual ambiguity problem, 
              we propose gated cross-lingual attention to exploit the complement information conveyed by multilingual data, which is
               helpful for the disambiguation. The cross-lingual attention gate serves as a sentinel modelling the confidence of the
                clues provided by other languages and controls the information integration of various languages. We have conducted 
                extensive experiments on the ACE 2005 benchmark. Experimental results show that our approach significantly outperforms
                state-of-the-art methods.},
 keywords = {event detection; attention mechanism; deep learning},
}

@inproceedings{NguyenAndGrishman:2018:AAAI2018,
 author = {Nguyen, Thien Huu and Grishman, Ralph},
 title = {Graph Convolutional Networks with Argument-Aware Pooling for Event Detection},
 booktitle = {The Thirty-Second AAAI Conference on Artificial Intelligence},
 series = {AAAI '2018},
 year = {2018},
 location = {New Orleans, Louisiana, USA},
 pages = {5900--5907},
 url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16329/16155},
 publisher = {Association for the Advancement of Artificial Intelligence},
 address = {New York, NY, USA},
 abstract = {The current neural network models for event detection have only considered the sequential 
 representation of sentences. Syntactic representations have not been explored in this area although 
 they provide an effective mechanism to directly link words to their informative context for event 
 detection in the sentences. In this work, we investigate a convolutional neural network based on 
 dependency trees to perform event detection. We propose a novel pooling method that relies on entity 
 mentions to aggregate the convolution vectors. The extensive experiments demonstrate the benefits of 
 the dependency-based convolutional neural networks and the entity mention- based pooling method for 
 event detection. We achieve the state-of-the-art performance on widely used datasets with both perfect 
 and predicted entity mentions.},
 keywords = {Event Detection; Information Extraction; Deep Learning; Graph Convolutional Neural Networks},
}

@inproceedings{MiyanishiEtAl:2018:AAAI2018,
 author = {Miyanishi, Taiki and Hirayama, Jun-ichiro and Maekawa, Takuya and Kawanabe, Motoaki},
 title = {Generating an Event Timeline about Daily Activities from a Semantic Concept Stream},
 booktitle = {The Thirty-Second AAAI Conference on Artificial Intelligence},
 series = {AAAI '2018},
 year = {2018},
 location = {New Orleans, Louisiana, USA},
 pages = {142--150},
 url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17197/15681},
 publisher = {Association for the Advancement of Artificial Intelligence},
 address = {New York, NY, USA},
 abstract = {Recognizing activities of daily living (ADLs) in the real world is an important task for understanding 
 everyday human life. However, even though our life events consist of chronological ADLs with the corresponding places 
 and objects (e.g., drinking coffee in the living room after making coffee in the kitchen and walking to the living 
 room), most existing works focus on predicting individual activity labels from sensor data. In this paper, we introduce 
 a novel framework that produces an event timeline of ADLs in a home environment. The proposed method combines semantic 
 concepts such as action, object, and place detected by sensors for generating stereotypical event sequences with the 
 following three real-world properties. First, we use temporal interactions among concepts to remove objects and places 
 unrelated to each action. Second, we use commonsense knowledge mined from a language resource to find a possible 
 combination of concepts in the real world. Third, we use temporal variations of events to filter repetitive events, 
 since our daily life changes over time. We use cross-place validation to evaluate our proposed method on a 
 daily-activities dataset with manually labeled event descriptions. The empirical evaluation demonstrates that our 
 method using real-world properties improves the performance of generating an event timeline over diverse environments.},
 keywords = {Activity Recognition; Event Timeline Generation; Activities of Daily Living; Semantic Concept Stream},
}

@inproceedings{MartinEtAl:2018:AAAI2018,
 author = {Martin, Lara J. and Ammanabrolu, Prithviraj and Wang, Xinyu and Hancock, William and Singh, Shruti and Harrison, Brent and Riedl, Mark O.},
 title = {Event Representations for Automated Story Generation with Deep Neural Nets},
 booktitle = {The Thirty-Second AAAI Conference on Artificial Intelligence},
 series = {AAAI '2018},
 year = {2018},
 location = {New Orleans, Louisiana, USA},
 pages = {868-875},
 url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17046/15769},
 publisher = {Association for the Advancement of Artificial Intelligence},
 address = {New York, NY, USA},
 abstract = {Automated story generation is the problem of automatically selecting a sequence of events, 
 actions, or words that can be told as a story. We seek to develop a system that can generate stories by 
 learning everything it needs to know from textual story corpora. To date, recurrent neural networks that 
 learn language models at character, word, or sentence levels have had little success generating coherent 
 stories. We explore the question of event representations that provide a mid-level of abstraction between 
 words and sentences in order to retain the semantic information of the original data while minimizing 
 event sparsity. We present a technique for preprocessing textual story data into event sequences. We then 
 present a technique for automated story generation whereby we decompose the problem into the generation 
 of successive events (event2event) and the generation of natural language sentences from events (event2sentence). 
 We give empirical results comparing different event representations and their effects on event successor 
 generation and the translation of events to natural language.},
 keywords = {automated story generation; event representations; recurrent neural networks},
}

@inproceedings{DingAndRiloff:2018:AAAI2018,
 author = {Ding, Haibo and Riloff Ellen},
 title = {Weakly Supervised Induction of Affective Events by Optimizing Semantic Consistency},
 booktitle = {The Thirty-Second AAAI Conference on Artificial Intelligence},
 series = {AAAI '2018},
 year = {2018},
 location = {New Orleans, Louisiana, USA},
 pages = {5763--5770},
 url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17097/16138},
 publisher = {Association for the Advancement of Artificial Intelligence},
 address = {New York, NY, USA},
 abstract = {To understand narrative text, we must comprehend how people are affected by the events that they experience. 
 For example, readers understand that graduating from college is a positive event (achievement) but being fired from one’s 
 job is a negative event (problem). NLP researchers have developed effective tools for recognizing explicit sentiments, 
 but affective events are more difficult to recognize because the polarity is often implicit and can depend on both a
  predicate and its arguments. Our research investigates the prevalence of affective events in a personal story corpus, 
  and introduces a weakly supervised method for large scale induction of affec- tive events. We present an iterative learning 
  framework that constructs a graph with nodes representing events and initial- izes their affective polarities with 
  sentiment analysis tools as weak supervision. The events are then linked based on three types of semantic relations: 
  (1) semantic similarity, (2) se- mantic opposition, and (3) shared components. The learning algorithm iteratively 
  refines the polarity values by optimiz- ing semantic consistency across all events in the graph. Our model learns over 
  100,000 affective events and identifies their polarities more accurately than other methods.
},
 keywords = {Natural Language Processing; Sentiment Analysis; Affective Events},
}

@inproceedings{LeeAndGoldwasser:2018:AAAI2018,
 author = {Lee, I-Ta and Goldwasser, Dan},
 title = {FEEL: Featured Event Embedding Learning},
 booktitle = {The Thirty-Second AAAI Conference on Artificial Intelligence},
 series = {AAAI '2018},
 year = {2018},
 location = {New Orleans, Louisiana, USA},
 pages = {4840--4847},
 url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17439/16014},
 publisher = {Association for the Advancement of Artificial Intelligence},
 address = {New York, NY, USA},
 abstract = {Statistical script learning is an effective way to acquire world knowledge which can be used for commonsense reasoning. 
 Statistical script learning induces this knowledge by observing event sequences generated from texts. The learned model thus can 
 predict subsequent events, given earlier events. Recent approaches rely on learning event embeddings which capture script knowledge. 
 In this work, we suggest a general learning model–Featured Event Embedding Learning (FEEL)–for injecting event embeddings with fine
  grained information. In addition to capturing the dependencies between subsequent events, our model can take into account higher 
  level abstractions of the input event which help the model generalize better and account for the global context in which the event 
  appears. We evaluated our model over three narrative cloze tasks, and showed that our model is competitive with the most recent 
  state-of-the-art. We also show that our resulting embedding can be used as a strong representation for advanced semantic tasks such
   as discourse parsing and sentence semantic relatedness.
},
 keywords = {Natural Language Processing; Event Embeddings; Common Sense Inference; Statistical Script Learning; Representation Learning},
}

@inproceedings{sha-Etal:2018:AAAI2018,
 author = {Sha, Lei and Qian, Feng and Chang, Baobao and Sui, Zhifang},
 title = {Jointly Extracting Event Triggers and Arguments by Dependency-Bridge RNN and Tensor-Based Argument Interaction},
 booktitle = {The Thirty-Second AAAI Conference on Artificial Intelligence},
 series = {AAAI '2018},
 year = {2018},
 location = {New Orleans, Louisiana, USA},
 pages = {5916-5923},
 url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16222/16157},
 publisher = {Association for the Advancement of Artificial Intelligence},
 address = {New York, NY, USA},
 abstract = {Event extraction plays an important role in natural language processing (NLP) applications 
 including question answering and information retrieval. Traditional event extraction relies heavily on 
 lexical and syntactic features, which require intensive human engineering and may not generalize to 
 different datasets. Deep neural networks, on the other hand, are able to automatically learn underlying 
 features, but existing networks do not make full use of syntactic relations. In this paper, we propose 
 a novel dependency bridge recurrent neural network (dbRNN) for event extraction. We build our model upon 
 a recurrent neural network, but enhance it with dependency bridges, which carry syntactically related 
 information when modeling each word. We illustrates that simultaneously applying tree structure and 
 sequence structure in RNN brings much better performance than only uses sequential RNN. In addition, 
 we use a tensor layer to simultaneously capture the various types of latent interaction between candidate 
 arguments as well as identify/classify all arguments of an event. Experiments show that our approach 
 achieves competitive re- sults compared with previous work.},
 keywords = {event; extraction; dependency bridge; tensor; joint},
}

@inproceedings{Ferguson-Etal:2018:NAACL2018,
 author = {Ferguson, James
		and Lockard, Colin
		and Weld, Daniel
		and Hajishirzi, Hannaneh},
 title = {Semi-Supervised Event Extraction with Paraphrase Clusters},
 booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
 series = {NAACL '2018},
 year = {2018},
 location = {New Orleans, Louisiana, USA},
 pages = {359--364},
 url = {http://aclweb.org/anthology/N18-2058},
 publisher = {Association for Computational Linguistics},
 abstract = {Supervised event extraction systems are limited in their accuracy due to the lack of available 
 training data. We present a method for self-training event extraction systems by bootstrapping additional 
 training data. This is done by taking advantage of the occurrence of multiple mentions of the same event 
 instances across newswire articles from multiple sources. If our system can make a highconﬁdence extraction 
 of some mentions in such a cluster, it can then acquire diverse training examples by adding the other mentions 
 as well. Our experiments show signiﬁcant performance improvements on multiple event extractors over ACE 2005 
 and TAC-KBP 2015 datasets.},
}

@InProceedings{lin-Etal-NPN:2018:ACL2018,
  author    = {Lin, Hongyu
    and Lu, Yaojie
    and Han, Xianpei
    and Sun, Le},
  title     = {Nugget Proposal Networks for Chinese Event Detection},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  month     = {July},
  year      = {2018},
  location  = {Melbourne, Australia},
  pages     = {1565--1574},
  publisher = {Association for Computational Linguistics},
  url = {http://aclweb.org/anthology/P18-1145},
}

@InProceedings{lin-Etal-AS:2018:ACL2018,
  author    = {Lin, Hongyu
    and Lu, Yaojie
    and Han, Xianpei
    and Sun, Le},
  title     = {Adaptive Scaling for Sparse Detection in Information Extraction},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  month     = {July},
  year      = {2018},
  location  = {Melbourne, Australia},
  pages     = {1033–-1043},
  publisher = {Association for Computational Linguistics},
  url = {http://aclweb.org/anthology/P18-1095},
}

@InProceedings{YaoAndHuang:2018:ACL2018,
  author    = {Yao, Wenlin
    and Huang, Ruihong},
  title     = {Temporal Event Knowledge Acquisition via Identifying Narratives},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  month     = {July},
  year      = {2018},
  location  = {Melbourne, Australia},
  pages     = {537–-547},
  publisher = {Association for Computational Linguistics},
  url = {http://aclweb.org/anthology/P18-1050},
}

@InProceedings{hong-Etal:2018:ACL2018,
  author    = {Hong, Yu
    and Zhou, Wenxuan
    and Zhang, Jingli
    and Zhu, Qiaoming
    and Zhou, Guodong},
  title     = {Self-regulation: Employing a Generative Adversarial Network to Improve Event Detection},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},
  month     = {July},
  year      = {2018},
  location  = {Melbourne, Australia},
  pages     = {515--526},
  publisher = {Association for Computational Linguistics},
  url = {http://aclweb.org/anthology/P18-1048},
}

@Article{Nils-and-Nazanin-and-Iryna:2018:TACL2018,
  author = 	"Reimers, Nils
		and Dehghani, Nazanin
		and Gurevych, Iryna",
  title = 	"Event Time Extraction with a Decision Tree of Neural Classifiers",
  journal = 	"Transactions of the Association for Computational Linguistics",
  year = 	"2018",
  volume = 	"6",
  pages = 	"77--89",
  url = 	"http://aclweb.org/anthology/Q18-1006",
}

@InProceedings{Chen-Etal:2018:EMNLP2018,
  author = 	"Chen, Yubo
		and Yang, Hang
		and Liu, Kang
		and Zhao, Jun
		and Jia, Yantao",
  title = 	"Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1267--1276",
  location = 	"Brussels, Belgium",
  
  ta
  url = 	"http://aclweb.org/anthology/D18-1158"
}

@InProceedings{Liu-Etal:2018:EMNLP2018,
  author = 	"Liu, Zhengzhong
		and Xiong, Chenyan
		and Mitamura, Teruko
		and Hovy, Eduard",
  title = 	"Automatic Event Salience Identification",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1226--1236",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1154"
}

@InProceedings{Liu-and-Luo-and-Huang:2018:EMNLP2018,
  author = 	"Liu, Xiao
		and Luo, Zhunchen
		and Huang, Heyan",
  title = 	"Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1247--1256",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1156"
}

@InProceedings{D18-1127,
  author = 	"LiuShaoBo-Etal:2018:EMNLP2018Short",
  title = 	"Exploiting Contextual Information via Dynamic Memory Network for Event Detection",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1030--1035",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1127"
}

@InProceedings{Orr-and-Tadepalli-and-Fern:2018:EMNLP2018Short,
  author = 	"Orr, Walker
		and Tadepalli, Prasad
		and Fern, Xiaoli",
  title = 	"Event Detection with Neural Networks: A Rigorous Empirical Evaluation",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"999--1004",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1122"
}

@InProceedings{Lu-and-Nguyen:2018:EMNLP2018Short,
  author = 	"Lu, Weiyi
		and Nguyen, Thien Huu",
  title = 	"Similar but not the Same - Word Sense Disambiguation Improves Event Detection via Neural Representation Matching",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"4822--4828",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1517"
}
