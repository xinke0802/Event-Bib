@inproceedings{Zhang:2017:MM,
 author = {Zhang, Tongtao and Whitehead, Spencer and Zhang, Hanwang and Li, Hongzhi and Ellis, Joseph and Huang, Lifu and Liu, Wei and Ji, Heng and Chang, Shih-Fu},
 title = {Improving Event Extraction via Multimodal Integration},
 booktitle = {Proceedings of the 2017 ACM on Multimedia Conference},
 series = {MM '17},
 year = {2017},
 isbn = {978-1-4503-4906-2},
 location = {Mountain View, California, USA},
 pages = {270--278},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/3123266.3123294},
 doi = {10.1145/3123266.3123294},
 acmid = {3123294},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {event extraction, multimodal approach, natural language processing, visual pattern discovery},
}

@InProceedings{chen-EtAl:2017:ACL,
  author    = {Chen, Yubo  and  Liu, Shulin  and  Zhang, Xiang  and  Liu, Kang  and  Zhao, Jun},
  title     = {Automatically Labeled Data Generation for Large Scale Event Extraction},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {409--419},
  abstract  = {Modern models of event extraction for tasks like ACE are based on supervised
	learning of events from small hand-labeled data. However, hand-labeled training
	data is expensive to produce, in low coverage of event types, and limited in
	size, which makes supervised methods hard to extract large scale of events for
	knowledge base population. To solve the data labeling problem, we propose to
	automatically label training data for event extraction via world knowledge and
	linguistic knowledge, which can detect key arguments and trigger words for each
	event type and employ them to label events in texts automatically. The
	experimental results show that the quality of our large scale automatically
	labeled data is competitive with elaborately human-labeled data. And our
	automatically labeled data can incorporate with human-labeled data, then
	improve the performance of models learned from these data.},
  url       = {http://aclweb.org/anthology/P17-1038}
}

@InProceedings{liu-EtAl:2017:ACL,
  author    = {Liu, Shulin  and  Chen, Yubo  and  Liu, Kang  and  Zhao, Jun},
  title     = {Exploiting Argument Information to Improve Event Detection via Supervised Attention Mechanisms},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1789--1798},
  abstract  = {This paper tackles the task of event detection (ED), which involves identifying
	and categorizing events. We argue that arguments provide significant clues to
	this task, but they are either completely ignored or exploited in an indirect
	manner in existing detection approaches. In this work, we propose to exploit
	argument information explicitly for ED via supervised attention mechanisms. In
	specific, we systematically investigate the proposed model under the
	supervision of different attention strategies. Experimental results show that
	our approach advances state-of-the-arts and achieves the best F1 score on
	ACE 2005 dataset.},
  url       = {http://aclweb.org/anthology/P17-1164}
}


@InProceedings{yang-mitchell:2017:Long,
  author    = {Yang, Bishan  and  Mitchell, Tom},
  title     = {Leveraging Knowledge Bases in LSTMs for Improving Machine Reading},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1436--1446},
  abstract  = {This paper focuses on how to take advantage of external knowledge bases (KBs)
	to improve recurrent neural networks for machine reading. Traditional methods
	that exploit knowledge from KBs encode knowledge as discrete indicator
	features. Not only do these features generalize poorly, but they require
	task-specific feature engineering to achieve good performance. We propose
	KBLSTM, a novel neural model that leverages continuous representations of KBs
	to enhance the learning of recurrent neural networks for machine reading. To
	effectively integrate background knowledge with information from the currently
	processed text, our model employs an attention mechanism with a sentinel to
	adaptively decide whether to attend to background knowledge and which
	information from KBs is useful. Experimental results show that our model
	achieves accuracies that surpass the previous state-of-the-art results for both
	entity extraction and event extraction on the widely used ACE2005 dataset.},
  url       = {http://aclweb.org/anthology/P17-1132}
}
